{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Sampling densities\n\nA collection of sampling densities and density-based non-Cartesian trajectories.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this example we illustrate the use of different sampling densities,\nand show how to generate trajectories based on them, such as random\nwalks and travelling-salesman trajectories.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# External\nimport brainweb_dl as bwdl\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom utils import (\n    show_density,\n    show_densities,\n    show_locations,\n    show_trajectory,\n    show_trajectories,\n)\n\n# Internal\nimport mrinufft as mn\nfrom mrinufft import display_2D_trajectory, display_3D_trajectory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Script options\n\nThese options are used in the examples below as default values for\nall densities and trajectories.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Density parameters\nshape_2d = (100, 100)\nshape_3d = (100, 100, 100)\n\n# Trajectory parameters\nNc = 10  # Number of shots\nNs = 50  # Number of samples per shot\n\n# Display parameters\nfigure_size = 5.5  # Figure size for trajectory plots\nsubfigure_size = 3  # Figure size for subplots\none_shot = 0  # Highlight one shot in particular"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Densities\n\nIn this section we present different sampling densities\nwith various properties.\n\n### Cutoff/decay density\n\nCreate a density composed of a central constant-value ellipsis\ndefined by a cutoff ratio, followed by a polynomial decay over\nouter regions as defined in [Cha+22]_.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "cutoff_density = mn.create_cutoff_decay_density(shape=shape_2d, cutoff=0.2, decay=2)\nshow_density(cutoff_density, figure_size=figure_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ``cutoff (float)``\n\nThe k-space radius cutoff ratio between 0 and 1 within\nwhich density remains uniform and beyond which it decays.\nIt is modulated by ``resolution`` to create ellipsoids.\n\nThe ``mrinufft.create_polynomial_density``\nsimply calls this function with ``cutoff=0``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "arguments = [0, 0.1, 0.2, 0.3]\nfunction = lambda x: mn.create_cutoff_decay_density(\n    shape=shape_2d,\n    cutoff=x,\n    decay=2,\n)\nshow_densities(\n    function,\n    arguments,\n    subfig_size=subfigure_size,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ``decay (float)``\n\nThe polynomial decay in density beyond the cutoff ratio.\nIt can be zero or negative as shown below, but most applications\nare expected have decays in the positive range.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "arguments = [-1, 0, 0.5, 2]\nfunction = lambda x: mn.create_cutoff_decay_density(\n    shape=shape_2d,\n    cutoff=0.2,\n    decay=x,\n)\nshow_densities(\n    function,\n    arguments,\n    subfig_size=subfigure_size,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ``resolution (tuple)``\n\nResolution scaling factors for each dimension of the density grid,\nby default ``None``. Note on the example below that the unit doesn't\nmatter because ``cutoff`` is a ratio and ``decay`` is an exponent,\nso only the relative factor between the dimensions is important.\n\nThis argument can be used to handle anisotropy but also to produce\nellipsoidal densities.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "arguments = [None, (1, 1), (1, 2), (1e-3, 0.5e-3)]\nfunction = lambda x: mn.create_cutoff_decay_density(\n    shape=shape_2d,\n    cutoff=0.2,\n    decay=2,\n    resolution=x,\n)\nshow_densities(\n    function,\n    arguments,\n    subfig_size=subfigure_size,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Energy-based density\n\nA common intuition is to consider that the sampling density\nshould be proportional to the k-space amplitude. It can be\nlearned from existing datasets and used for new acquisitions.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dataset = bwdl.get_mri(4, \"T1\")[:, ::2, ::2]\nenergy_density = mn.create_energy_density(dataset=dataset)\nshow_density(energy_density, figure_size=figure_size, log_scale=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ``dataset (np.ndarray)``\n\nThe dataset from which to calculate the density\nbased on its Fourier transform, with an expected\nshape (nb_volumes, dim_1, ..., dim_N).\nAn N-dimensional Fourier transform is then performed.\n\nIn the example below, we show the resulting densities\nfrom different slices of a single volume for convenience.\nMore relevant use cases would be to learn densities for\ndifferent organs and/or contrasts.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "arguments = [50, 100, 150]\nfunction = lambda x: mn.create_energy_density(dataset=bwdl.get_mri(4, \"T1\")[x : x + 20])\nshow_densities(\n    function,\n    arguments,\n    subfig_size=subfigure_size,\n    log_scale=True,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Chauffert's density\n\nThis is a reproduction of the proposition from [CCW13]_.\nA sampling density is derived from compressed sensing\nequations to maximize guarantees of exact image recovery\nfor a specified sparse wavelet domain decomposition.\n\nThis principle is valid for any linear transform but\nfor convenience it was limited to wavelets as in the\noriginal implementation.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "chauffert_density = mn.create_chauffert_density(\n    shape=shape_2d,\n    wavelet_basis=\"haar\",\n    nb_wavelet_scales=3,\n)\nshow_density(chauffert_density, figure_size=figure_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ``wavelet_basis (str)``\n\nThe wavelet basis to use for wavelet decomposition, either\nas a built-in wavelet name from the PyWavelets package\nor as a custom ``pywt.Wavelet`` object.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "arguments = [\"haar\", \"rbio2.2\", \"coif4\", \"sym8\"]\nfunction = lambda x: mn.create_chauffert_density(\n    shape=shape_2d,\n    wavelet_basis=x,\n    nb_wavelet_scales=3,\n)\nshow_densities(\n    function,\n    arguments,\n    subfig_size=subfigure_size,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ``nb_wavelet_scales (int)``\n\nThe number of wavelet scales to use in decomposition.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "arguments = [1, 2, 3, 4]\nfunction = lambda x: mn.create_chauffert_density(\n    shape=shape_2d,\n    wavelet_basis=\"haar\",\n    nb_wavelet_scales=x,\n)\nshow_densities(\n    function,\n    arguments,\n    subfig_size=subfigure_size,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Custom density\n\nAny density can be defined and later used for sampling and\ntrajectories.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Linear gradient\ndensity = np.tile(np.linspace(0, 1, shape_2d[1])[:, None], (1, shape_2d[0]))\n# Square center\ndensity[\n    3 * shape_2d[0] // 8 : 5 * shape_2d[0] // 8,\n    3 * shape_2d[1] // 8 : 5 * shape_2d[1] // 8,\n] = 2\n# Outer circle\ndensity = np.where(\n    np.linalg.norm(np.indices(shape_2d) - shape_2d[0] / 2, axis=0) < shape_2d[0] / 2,\n    density,\n    0,\n)\n# Normalization\ncustom_density = density / np.sum(density)\n\nshow_density(custom_density, figure_size=figure_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sampling\n\nIn this section we present random, pseudo-random and\nalgorithm-based sampling methods. The examples are based\non a few densities picked from the ones presented above.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "densities = {\n    \"Cutoff/Decay\": cutoff_density,\n    \"Energy\": energy_density,\n    \"Chauffert\": chauffert_density,\n    \"Custom\": custom_density,\n}\n\narguments = densities.keys()\nfunction = lambda x: densities[x]\nshow_densities(function, arguments, subfig_size=subfigure_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Random sampling\n\nThis sampling simply consists of weighted-random selection from the\ndensity grid locations.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "arguments = densities.keys()\nfunction = lambda x: mn.sample_from_density(Nc * Ns, densities[x], method=\"random\")\nshow_locations(function, arguments, subfig_size=subfigure_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Lloyd's sampling\n\nThis sampling is based on a Voronoi/Dirichlet tesselation using Lloyd's\nweighted KMeans algorithm. The implementation is based on\n``sklearn.cluster.KMeans`` in 2D and ``sklearn.cluster.BisectingKMeans``\nin 3D, mostly to reduce computation times in the most demanding cases.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "arguments = densities.keys()\nfunction = lambda x: mn.sample_from_density(Nc * Ns, densities[x], method=\"lloyd\")\nshow_locations(function, arguments, subfig_size=subfigure_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Density-based trajectories\n\nIn this section we present 2D and 3D trajectories based\non arbitrary densities, and also sampling for some of them.\n\n### Random walks\n\nThis is an adaptation of the proposition from [Cha+14]_.\nIt creates a trajectory by walking randomly to neighboring points\nfollowing a provided sampling density.\n\nThis implementation is different from the original proposition:\ntrajectories are continuous with a fixed length instead of\nmaking random jumps to other locations, and an option\nis provided to have pseudo-random walks to improve coverage.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "arguments = densities.keys()\nfunction = lambda x: mn.initialize_2D_random_walk(\n    Nc, Ns, density=densities[x][::4, ::4]\n)\nshow_trajectories(function, arguments, one_shot=one_shot, subfig_size=subfigure_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The starting shot positions can be modified to follow Lloyd's sampling\nmethod rather than the default random approach, resulting in more evenly\nspaced shots that still respect the prescribed density.\nAdditional ``kwargs`` can provided to set the arguments in\n``mrinufft.sample_from_density``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "arguments = densities.keys()\nfunction = lambda x: mn.initialize_2D_random_walk(\n    Nc, Ns, density=densities[x][::4, ::4], method=\"lloyd\"\n)\nshow_trajectories(function, arguments, one_shot=one_shot, subfig_size=subfigure_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The random paths can be made into a smooth and continuous\ntrajectory by oversampling the shots with cubic splines.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "arguments = densities.keys()\nfunction = lambda x: mn.oversample(\n    mn.initialize_2D_random_walk(\n        Nc, Ns, density=densities[x][::4, ::4], method=\"lloyd\"\n    ),\n    4 * Ns,\n)\nshow_trajectories(function, arguments, one_shot=one_shot, subfig_size=subfigure_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Travelling Salesman\n\nThis is a reproduction of the work from [Cha+14]_. The Travelling\nSalesman Problem (TSP) solution is obtained using the 2-opt method\nwith a complexity in O(n\u00b2) in time and memory.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "arguments = densities.keys()\nfunction = lambda x: mn.initialize_2D_travelling_salesman(\n    Nc,\n    Ns,\n    density=densities[x],\n)\nshow_trajectories(function, arguments, one_shot=one_shot, subfig_size=subfigure_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It is possible to customize the sampling method using ``kwargs``\nto provide arguments to ``mrinufft.sample_from_density``.\nFor example, one can use Lloyd's sampling method to create evenly\nspaced point distributions and obtain a more deterministic coverage.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "arguments = densities.keys()\nfunction = lambda x: mn.initialize_2D_travelling_salesman(\n    Nc,\n    Ns,\n    density=densities[x],\n    method=\"lloyd\",\n)\nshow_trajectories(function, arguments, one_shot=one_shot, subfig_size=subfigure_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Similarly to random walks, the travelling paths can be smoothed\nby oversampling the shots with cubic splines. Another use case\nis to reduce the number of TSP points to reduce the computation load\nand then oversample up to the desired shot length.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "arguments = densities.keys()\nfunction = lambda x: mn.oversample(\n    mn.initialize_2D_travelling_salesman(Nc, Ns, density=densities[x], method=\"lloyd\"),\n    4 * Ns,\n)\nshow_trajectories(function, arguments, one_shot=one_shot, subfig_size=subfigure_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "An option is provided to cluster the points before calling the TSP solver,\nreducing drastically the computation time.\nClusters are chosen by Cartesian (``\"x\"``, ``\"y\"``, ``\"z\"``) or spherical\n(``\"r\"``, ``\"phi\"``, ``\"theta\"``) coordinate with up to two coordinates.\nThen the points can be sorted within each cluster in order to define a general\nshot direction as shown below.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "arguments = ((None, None, None), (\"y\", None, \"x\"), (\"phi\", None, \"r\"), (\"y\", \"x\", \"r\"))\nfunction = lambda x: mn.initialize_2D_travelling_salesman(\n    Nc,\n    Ns,\n    density=densities[\"Custom\"],\n    first_cluster_by=x[0],\n    second_cluster_by=x[1],\n    sort_by=x[2],\n    method=\"lloyd\",\n)\nshow_trajectories(function, arguments, one_shot=one_shot, subfig_size=subfigure_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References\n\n.. [CCW13] Chauffert, Nicolas, Philippe Ciuciu, and Pierre Weiss.\n   \"Variable density compressed sensing in MRI.\n   Theoretical vs heuristic sampling strategies.\"\n   In 2013 IEEE 10th International Symposium on Biomedical Imaging,\n   pp. 298-301. IEEE, 2013.\n.. [Cha+14] Chauffert, Nicolas, Philippe Ciuciu,\n   Jonas Kahn, and Pierre Weiss.\n   \"Variable density sampling with continuous trajectories.\"\n   SIAM Journal on Imaging Sciences 7, no. 4 (2014): 1962-1992.\n.. [Cha+22] Chaithya, G. R., Pierre Weiss, Guillaume Daval-Fr\u00e9rot,\n   Aur\u00e9lien Massire, Alexandre Vignaud, and Philippe Ciuciu.\n   \"Optimizing full 3D SPARKLING trajectories for high-resolution\n   magnetic resonance imaging.\"\n   IEEE Transactions on Medical Imaging 41, no. 8 (2022): 2105-2117.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}