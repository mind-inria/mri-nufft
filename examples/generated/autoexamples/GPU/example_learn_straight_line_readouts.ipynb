{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "!pip install mri-nufft[cufinufft,finufft,gpunufft,extra,autodiff]\n!pip install brainweb-dl fastmri"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Learn Straight line readout pattern\n\nA small pytorch example to showcase learning k-space sampling patterns.\nIn this example we learn the 2D sampling pattern for a 3D MRI image, assuming\nstraight line readouts. This example showcases the auto-diff capabilities of the NUFFT operator\nThe image resolution is kept small to reduce computation time.\n\n<div class=\"alert alert-danger\"><h4>Warning</h4><p>This example only showcases the autodiff capabilities, the learned\n    sampling pattern is not scanner compliant as the scanner gradients required\n    to implement it violate the hardware constraints. In practice, a projection\n    $\\Pi_\\mathcal{Q}(\\mathbf{K})$ into the scanner constraints set\n    $\\mathcal{Q}$ is recommended (see [Proj]_). This is implemented in the\n    proprietary SPARKLING package [Sparks]_. Users are encouraged to contact the\n    authors if they want to use it.</p></div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\n\nimport brainweb_dl as bwdl\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport matplotlib.animation as animation\n\nfrom mrinufft import get_operator\n\nBACKEND = os.environ.get(\"MRINUFFT_BACKEND\", \"cufinufft\")\n\nplt.rcParams[\"animation.embed_limit\"] = 2**30  # 1GiB is very large."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup a simple class to learn trajectory\n<div class=\"alert alert-info\"><h4>Note</h4><p>While we are only learning the NUFFT operator, we still need the gradient `wrt_data=True` to have all the gradients computed correctly.\n    See [Projector]_ for more details.</p></div>\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>Since we are training a 2D non-Cartesian pattern for a 3D volume, we could use the \"stacked\" version of\n    the operator, which uses a FFT instead of a NUFFT. However, this is not supported yet, so we use the full 3D implementation !</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Model(torch.nn.Module):\n    def __init__(self, num_shots, img_size, factor_cartesian=0.1):\n        super().__init__()\n        self.num_samples_per_shot = 128\n        cart_del = 1 / img_size[0]\n        num_cart_points = np.round(np.sqrt(factor_cartesian * num_shots)).astype(int)\n        edge_center = cart_del * num_cart_points / 2\n\n        self.central_points = torch.nn.Parameter(\n            data=torch.stack(\n                torch.meshgrid(\n                    torch.linspace(\n                        -edge_center, edge_center, num_cart_points, dtype=torch.float32\n                    ),\n                    torch.linspace(\n                        -edge_center, edge_center, num_cart_points, dtype=torch.float32\n                    ),\n                    indexing=\"ij\",\n                ),\n                axis=-1,\n            ).reshape(-1, 2),\n            requires_grad=False,\n        )\n        self.non_center_points = torch.nn.Parameter(\n            data=torch.Tensor(\n                np.random.random((num_shots - self.central_points.shape[0], 2)).astype(\n                    np.float32\n                )\n                - 0.5\n            ),\n            requires_grad=True,\n        )\n        self.operator = get_operator(BACKEND, wrt_data=True, wrt_traj=True)(\n            np.random.random(\n                (self.get_2D_points().shape[0] * self.num_samples_per_shot, 3)\n            ).astype(np.float32)\n            - 0.5,\n            shape=img_size,\n            density=True,\n            squeeze_dims=False,\n        )\n\n    def get_trajectory(self, get_as_shot=False):\n        samples = self._get_3D_points(self.get_2D_points())\n        if not get_as_shot:\n            return samples\n        return samples.reshape(-1, self.num_samples_per_shot, 3)\n\n    def get_2D_points(self):\n        return torch.vstack([self.central_points, self.non_center_points])\n\n    def _get_3D_points(self, samples2D):\n        line = torch.linspace(\n            -0.5,\n            0.5,\n            self.num_samples_per_shot,\n            device=samples2D.device,\n            dtype=samples2D.dtype,\n        )\n        return torch.stack(\n            [\n                line.repeat(samples2D.shape[0], 1),\n                samples2D[:, 0].repeat(self.num_samples_per_shot, 1).T,\n                samples2D[:, 1].repeat(self.num_samples_per_shot, 1).T,\n            ],\n            dim=-1,\n        ).reshape(-1, 3)\n\n    def forward(self, x):\n        self.operator.samples = self.get_trajectory()\n        kspace = self.operator.op(x)\n        adjoint = self.operator.adj_op(kspace).abs()\n        return adjoint / torch.mean(adjoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup model and optimizer\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "num_epochs = 100\n\ncart_data = np.flipud(bwdl.get_mri(4, \"T1\")).T[::8, ::8, ::8].astype(np.complex64)\nmodel = Model(253, cart_data.shape)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\nschedulder = torch.optim.lr_scheduler.LinearLR(\n    optimizer,\n    start_factor=1,\n    end_factor=0.01,\n    total_iters=num_epochs,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup data\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mri_3D = torch.Tensor(cart_data)[None]\nmri_3D = mri_3D / torch.mean(mri_3D)\nmodel.eval()\nrecon = model(mri_3D)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Start training loop\nRed points in the graph show the original locations, and the blue ones the new updated trajectory.\nAs training goes, they will deviate more and more.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(2, 2, figsize=(15, 15))\naxs = axs.ravel()\naxs[0].imshow(np.abs(mri_3D.squeeze())[..., 11], cmap=\"gray\")\naxs[0].axis(\"off\")\naxs[0].set_title(\"Ground truth\")\naxs[1].remove()\naxs[1] = fig.add_subplot(222, projection=\"3d\", azim=0, elev=0)\ntraj_scat = axs[1].scatter(\n    *model.get_trajectory(True).detach().cpu().numpy()[:, 0, :].T, s=1, c=\"tab:blue\"\n)\ntraj_scat2 = axs[1].scatter(\n    *model.get_trajectory(True).detach().cpu().numpy()[:, 0, :].T, s=1, c=\"tab:red\"\n)\naxs[1].set_xlim(-0.5, 0.5)\naxs[1].set_ylim(-0.5, 0.5)\naxs[1].set_zlim(-0.5, 0.5)\n# traj_scat, = axs[1].plot(*model.get_trajectory(True).detach().cpu().numpy()[:,0,:].T, linestyle=\"\", marker=\"o\")\naxs[1].set_title(\"Trajectory\")\n\nrecon_im = axs[2].imshow(\n    np.abs(recon.squeeze()[..., 11].detach().cpu().numpy()), cmap=\"gray\"\n)\naxs[2].set_title(\"Reconstruction\")\naxs[2].axis(\"off\")\n\naxs[3].grid()\n(loss_curve,) = axs[3].plot([], [])\naxs[3].set_ylabel(\"Loss\")\naxs[3].set_xlabel(\"epoch\")\nfig.suptitle(\"Starting Training\")\nfig.tight_layout()\n\n\ndef train():\n    \"\"\"Train loop.\"\"\"\n    losses = []\n    old_traj = None\n    for i in range(num_epochs):\n        out = model(mri_3D)\n        loss = torch.norm(out - mri_3D[None])  # Compute loss\n\n        optimizer.zero_grad()  # Zero gradients\n        loss.backward()  # Backward pass\n        optimizer.step()  # Update weights\n        with torch.no_grad():\n            # clamp the value of trajectory between [-0.5, 0.5]\n            for param in model.parameters():\n                param.clamp_(-0.5, 0.5)\n        schedulder.step()\n        losses.append(loss.item())\n        new_traj = model.get_trajectory(True).detach().cpu().numpy()\n        yield (\n            out.detach().cpu().numpy().squeeze()[..., 11],\n            new_traj,\n            old_traj,\n            losses,\n        )\n        old_traj = new_traj\n\n\ndef plot_epoch(data):\n    img, new_traj, old_traj, losses = data\n\n    cur_epoch = len(losses)\n    recon_im.set_data(abs(img))\n    loss_curve.set_xdata(np.arange(cur_epoch))\n    loss_curve.set_ydata(losses)\n    mov3d = 70\n    if cur_epoch > mov3d:\n        #        traj_scat2.set_offsets([[np.nan, np.nan]])\n        trajf = new_traj.reshape(-1, 3)\n        traj_scat.set_offsets(trajf[:, :2])\n        traj_scat.set_3d_properties(trajf[:, 2], \"z\", True)\n        # traj_scat.set_xdata(traj[:, :, 0].ravel())\n        # traj_scat.set_ydata(traj[:, :, 1].ravel())\n        # traj_scat.set_3d_properties(traj[:, :, 2].ravel())\n        axs[1].view_init(azim=(cur_epoch - mov3d), elev=(cur_epoch - mov3d))\n    else:\n        traj_scat.set_offsets(new_traj[:, 0, :2])\n\n    axs[3].set_xlim(0, cur_epoch)\n    axs[3].set_ylim(0, 1.1 * max(losses))\n    axs[2].set_title(f\"Reconstruction, frame {cur_epoch}/{num_epochs}\")\n    axs[1].set_title(f\"Trajectory, frame {cur_epoch}/{num_epochs}\")\n\n    if cur_epoch < num_epochs:\n        fig.suptitle(\"Training in progress \" + \".\" * (1 + cur_epoch % 3))\n    else:\n        fig.suptitle(\"Training complete !\")\n\n\nani = animation.FuncAnimation(\n    fig, plot_epoch, train, save_count=num_epochs, repeat=False\n)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### References\n\n.. [Proj] N. Chauffert, P. Weiss, J. Kahn and P. Ciuciu, \"A Projection Algorithm for\n          Gradient Waveforms Design in Magnetic Resonance Imaging,\" in\n          IEEE Transactions on Medical Imaging, vol. 35, no. 9, pp. 2026-2039, Sept. 2016,\n          doi: 10.1109/TMI.2016.2544251.\n.. [Sparks] G. R. Chaithya, P. Weiss, G. Daval-Fr\u00e9rot, A. Massire, A. Vignaud and P. Ciuciu,\n          \"Optimizing Full 3D SPARKLING Trajectories for High-Resolution Magnetic\n          Resonance Imaging,\" in IEEE Transactions on Medical Imaging, vol. 41, no. 8,\n          pp. 2105-2117, Aug. 2022, doi: 10.1109/TMI.2022.3157269.\n.. [Projector] Chaithya GR, and Philippe Ciuciu. 2023. \"Jointly Learning Non-Cartesian\n          k-Space Trajectories and Reconstruction Networks for 2D and 3D MR Imaging\n          through Projection\" Bioengineering 10, no. 2: 158.\n          https://doi.org/10.3390/bioengineering10020158\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}