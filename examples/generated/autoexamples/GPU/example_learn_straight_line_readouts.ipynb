{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class='alert alert-warning'>\n\n# Need GPU warning\n\nRunning this mri-nufft example requires a GPU, and hence is NOT possible on binder currently We request you to kindly run this notebook on Google Colab by clicking the link below. Additionally, please make sure to set the runtime on Colab to use a GPU and install the below libraries before running.\n</div>\n<div class=\"colab-button\">\n            <a href=\"https://colab.research.google.com/github/mind-inria/mri-nufft/blob/gh-pages/examples/generated/autoexamples/GPU/example_learn_straight_line_readouts.ipynb\" target=\"_blank\">\n                <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" \n                alt=\"Open In Colab\"/>\n            </a>\n        </div>\n        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Learn Straight line readout pattern\n\nA small pytorch example to showcase learning k-space sampling patterns.\nIn this example we learn the 2D sampling pattern for a 3D MRI image, assuming\nstraight line readouts. This example showcases the auto-diff capabilities of the NUFFT operator\nThe image resolution is kept small to reduce computation time.\n\n<div class=\"alert alert-danger\"><h4>Warning</h4><p>This example only showcases the autodiff capabilities, the learned\n    sampling pattern is not scanner compliant as the scanner gradients required\n    to implement it violate the hardware constraints. In practice, a projection\n    $\\Pi_\\mathcal{Q}(\\mathbf{K})$ into the scanner constraints set\n    $\\mathcal{Q}$ is recommended (see [Proj]_). This is implemented in the\n    proprietary SPARKLING package [Sparks]_. Users are encouraged to contact the\n    authors if they want to use it.</p></div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Install libraries\n!pip install mri-nufft[gpunufft]\n!pip install brainweb-dl  # Required for data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\n\nimport brainweb_dl as bwdl\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport matplotlib.animation as animation\n\nfrom mrinufft import get_operator\n\nBACKEND = os.environ.get(\"MRINUFFT_BACKEND\", \"cufinufft\")\n\nplt.rcParams[\"animation.embed_limit\"] = 2**30  # 1GiB is very large."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup a simple class to learn trajectory\n<div class=\"alert alert-info\"><h4>Note</h4><p>While we are only learning the NUFFT operator, we still need the gradient `wrt_data=True` to have all the gradients computed correctly.\n    See [Projector]_ for more details.</p></div>\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>Since we are training a 2D non-Cartesian pattern for a 3D volume, we could use the \"stacked\" version of\n    the operator, which uses a FFT instead of a NUFFT. However, this is not supported yet, so we use the full 3D implementation !</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Model(torch.nn.Module):\n    def __init__(self, num_shots, img_size, factor_cartesian=0.1):\n        super().__init__()\n        self.num_samples_per_shot = 128\n        cart_del = 1 / img_size[0]\n        num_cart_points = np.round(np.sqrt(factor_cartesian * num_shots)).astype(int)\n        edge_center = cart_del * num_cart_points / 2\n\n        self.central_points = torch.nn.Parameter(\n            data=torch.stack(\n                torch.meshgrid(\n                    torch.linspace(\n                        -edge_center, edge_center, num_cart_points, dtype=torch.float32\n                    ),\n                    torch.linspace(\n                        -edge_center, edge_center, num_cart_points, dtype=torch.float32\n                    ),\n                    indexing=\"ij\",\n                ),\n                axis=-1,\n            ).reshape(-1, 2),\n            requires_grad=False,\n        )\n        self.non_center_points = torch.nn.Parameter(\n            data=torch.Tensor(\n                np.random.random((num_shots - self.central_points.shape[0], 2)).astype(\n                    np.float32\n                )\n                - 0.5\n            ),\n            requires_grad=True,\n        )\n        self.operator = get_operator(BACKEND, wrt_data=True, wrt_traj=True)(\n            np.random.random(\n                (self.get_2D_points().shape[0] * self.num_samples_per_shot, 3)\n            ).astype(np.float32)\n            - 0.5,\n            shape=img_size,\n            density=True,\n            squeeze_dims=False,\n        )\n\n    def get_trajectory(self, get_as_shot=False):\n        samples = self._get_3D_points(self.get_2D_points())\n        if not get_as_shot:\n            return samples\n        return samples.reshape(-1, self.num_samples_per_shot, 3)\n\n    def get_2D_points(self):\n        return torch.vstack([self.central_points, self.non_center_points])\n\n    def _get_3D_points(self, samples2D):\n        line = torch.linspace(\n            -0.5,\n            0.5,\n            self.num_samples_per_shot,\n            device=samples2D.device,\n            dtype=samples2D.dtype,\n        )\n        return torch.stack(\n            [\n                line.repeat(samples2D.shape[0], 1),\n                samples2D[:, 0].repeat(self.num_samples_per_shot, 1).T,\n                samples2D[:, 1].repeat(self.num_samples_per_shot, 1).T,\n            ],\n            dim=-1,\n        ).reshape(-1, 3)\n\n    def forward(self, x):\n        self.operator.samples = self.get_trajectory()\n        kspace = self.operator.op(x)\n        adjoint = self.operator.adj_op(kspace).abs()\n        return adjoint / torch.mean(adjoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup model and optimizer\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "num_epochs = 100\n\ncart_data = np.flipud(bwdl.get_mri(4, \"T1\")).T[::8, ::8, ::8].astype(np.complex64)\nmodel = Model(253, cart_data.shape)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\nschedulder = torch.optim.lr_scheduler.LinearLR(\n    optimizer,\n    start_factor=1,\n    end_factor=0.01,\n    total_iters=num_epochs,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup data\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mri_3D = torch.Tensor(cart_data)[None]\nmri_3D = mri_3D / torch.mean(mri_3D)\nmodel.eval()\nrecon = model(mri_3D)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Start training loop\nRed points in the graph show the original locations, and the blue ones the new updated trajectory.\nAs training goes, they will deviate more and more.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(2, 2, figsize=(15, 15))\naxs = axs.ravel()\naxs[0].imshow(np.abs(mri_3D.squeeze())[..., 11], cmap=\"gray\")\naxs[0].axis(\"off\")\naxs[0].set_title(\"Ground truth\")\naxs[1].remove()\naxs[1] = fig.add_subplot(222, projection=\"3d\", azim=0, elev=0)\ntraj_scat = axs[1].scatter(\n    *model.get_trajectory(True).detach().cpu().numpy()[:, 0, :].T, s=1, c=\"tab:blue\"\n)\ntraj_scat2 = axs[1].scatter(\n    *model.get_trajectory(True).detach().cpu().numpy()[:, 0, :].T, s=1, c=\"tab:red\"\n)\naxs[1].set_xlim(-0.5, 0.5)\naxs[1].set_ylim(-0.5, 0.5)\naxs[1].set_zlim(-0.5, 0.5)\n# traj_scat, = axs[1].plot(*model.get_trajectory(True).detach().cpu().numpy()[:,0,:].T, linestyle=\"\", marker=\"o\")\naxs[1].set_title(\"Trajectory\")\n\nrecon_im = axs[2].imshow(\n    np.abs(recon.squeeze()[..., 11].detach().cpu().numpy()), cmap=\"gray\"\n)\naxs[2].set_title(\"Reconstruction\")\naxs[2].axis(\"off\")\n\naxs[3].grid()\n(loss_curve,) = axs[3].plot([], [])\naxs[3].set_ylabel(\"Loss\")\naxs[3].set_xlabel(\"epoch\")\nfig.suptitle(\"Starting Training\")\nfig.tight_layout()\n\n\ndef train():\n    \"\"\"Train loop.\"\"\"\n    losses = []\n    old_traj = None\n    for i in range(num_epochs):\n        out = model(mri_3D)\n        loss = torch.norm(out - mri_3D[None])  # Compute loss\n\n        optimizer.zero_grad()  # Zero gradients\n        loss.backward()  # Backward pass\n        optimizer.step()  # Update weights\n        with torch.no_grad():\n            # clamp the value of trajectory between [-0.5, 0.5]\n            for param in model.parameters():\n                param.clamp_(-0.5, 0.5)\n        schedulder.step()\n        losses.append(loss.item())\n        new_traj = model.get_trajectory(True).detach().cpu().numpy()\n        yield (\n            out.detach().cpu().numpy().squeeze()[..., 11],\n            new_traj,\n            old_traj,\n            losses,\n        )\n        old_traj = new_traj\n\n\ndef plot_epoch(data):\n    img, new_traj, old_traj, losses = data\n\n    cur_epoch = len(losses)\n    recon_im.set_data(abs(img))\n    loss_curve.set_xdata(np.arange(cur_epoch))\n    loss_curve.set_ydata(losses)\n    mov3d = 70\n    if cur_epoch > mov3d:\n        #        traj_scat2.set_offsets([[np.nan, np.nan]])\n        trajf = new_traj.reshape(-1, 3)\n        traj_scat.set_offsets(trajf[:, :2])\n        traj_scat.set_3d_properties(trajf[:, 2], \"z\", True)\n        # traj_scat.set_xdata(traj[:, :, 0].ravel())\n        # traj_scat.set_ydata(traj[:, :, 1].ravel())\n        # traj_scat.set_3d_properties(traj[:, :, 2].ravel())\n        axs[1].view_init(azim=(cur_epoch - mov3d), elev=(cur_epoch - mov3d))\n    else:\n        traj_scat.set_offsets(new_traj[:, 0, :2])\n\n    axs[3].set_xlim(0, cur_epoch)\n    axs[3].set_ylim(0, 1.1 * max(losses))\n    axs[2].set_title(f\"Reconstruction, frame {cur_epoch}/{num_epochs}\")\n    axs[1].set_title(f\"Trajectory, frame {cur_epoch}/{num_epochs}\")\n\n    if cur_epoch < num_epochs:\n        fig.suptitle(\"Training in progress \" + \".\" * (1 + cur_epoch % 3))\n    else:\n        fig.suptitle(\"Training complete !\")\n\n\nani = animation.FuncAnimation(\n    fig, plot_epoch, train, save_count=num_epochs, repeat=False\n)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### References\n\n.. [Proj] N. Chauffert, P. Weiss, J. Kahn and P. Ciuciu, \"A Projection Algorithm for\n          Gradient Waveforms Design in Magnetic Resonance Imaging,\" in\n          IEEE Transactions on Medical Imaging, vol. 35, no. 9, pp. 2026-2039, Sept. 2016,\n          doi: 10.1109/TMI.2016.2544251.\n.. [Sparks] G. R. Chaithya, P. Weiss, G. Daval-Frérot, A. Massire, A. Vignaud and P. Ciuciu,\n          \"Optimizing Full 3D SPARKLING Trajectories for High-Resolution Magnetic\n          Resonance Imaging,\" in IEEE Transactions on Medical Imaging, vol. 41, no. 8,\n          pp. 2105-2117, Aug. 2022, doi: 10.1109/TMI.2022.3157269.\n.. [Projector] Chaithya GR, and Philippe Ciuciu. 2023. \"Jointly Learning Non-Cartesian\n          k-Space Trajectories and Reconstruction Networks for 2D and 3D MR Imaging\n          through Projection\" Bioengineering 10, no. 2: 158.\n          https://doi.org/10.3390/bioengineering10020158\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}