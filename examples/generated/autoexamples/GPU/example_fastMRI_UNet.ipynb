{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "!pip install mri-nufft[cufinufft,finufft,gpunufft,extra,autodiff]\n!pip install brainweb-dl fastmri"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Simple UNet model.\n\nThis model is a simplified version of the U-Net architecture,\nwhich is widely used for image segmentation tasks.\nThis is implemented in the proprietary FASTMRI package [fastmri]_.\n\nThe U-Net model consists of an encoder (downsampling path) and\na decoder (upsampling path) with skip connections between corresponding\nlayers in the encoder and decoder.\nThese skip connections help in retaining spatial information\nthat is lost during the downsampling process.\n\nThe primary purpose of this model is to perform image reconstruction tasks,\nspecifically for MRI images.\nIt takes an input MRI image and reconstructs it to improve the image quality\nor to recover missing parts of the image.\n\nThis implementation of the UNet model was pulled from the FastMRI Facebook\nrepository, which is a collaborative research project aimed at advancing\nthe field of medical imaging using machine learning techniques.\n\n\\begin{align}\\mathbf{\\hat{x}} = \\mathrm{arg} \\min_{\\mathbf{x}} || \\mathcal{U}_\\mathbf{\\theta}(\\mathbf{y}) - \\mathbf{x} ||_2^2\\end{align}\n\nwhere $\\mathbf{\\hat{x}}$ is the reconstructed MRI image, $\\mathbf{x}$ is the ground truth image,\n$\\mathbf{y}$ is the input MRI image (e.g., k-space data), and $\\mathcal{U}_\\mathbf{\\theta}$ is the U-Net model parameterized by $\\theta$.\n\n<div class=\"alert alert-danger\"><h4>Warning</h4><p>We train on a single image here. In practice, this should be done on a database like fastMRI [fastmri]_.</p></div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Imports\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\nimport brainweb_dl as bwdl\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\n\nimport matplotlib.animation as animation\nfrom fastmri.models import Unet\nfrom mrinufft import get_operator\nfrom mrinufft.trajectories import initialize_2D_cones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Setup a simple class for the U-Net model\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "BACKEND = os.environ.get(\"MRINUFFT_BACKEND\", \"cufinufft\")\n\nplt.rcParams[\"animation.embed_limit\"] = 2**30  # 1GiB is very large.\n\n\nclass Model(torch.nn.Module):\n    \"\"\"Model for MRI reconstruction using a U-Net.\"\"\"\n\n    def __init__(self, initial_trajectory):\n        super().__init__()\n        self.operator = get_operator(BACKEND, wrt_data=True)(\n            initial_trajectory,\n            shape=(256, 256),\n            density=True,\n            squeeze_dims=False,\n        )\n        self.unet = Unet(in_chans=1, out_chans=1, chans=32, num_pool_layers=4)\n\n    def forward(self, kspace):\n        \"\"\"Forward pass of the model.\"\"\"\n        image = self.operator.adj_op(kspace)\n        recon = self.unet(image.float()).abs()\n        recon /= torch.mean(recon)\n        return recon"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Utility function to plot the state of the model\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot_state(axs, mri_2D, traj, recon, loss=None, save_name=None):\n    \"\"\"Image plotting function.\n\n    Plot the original MRI image, the trajectory, the reconstructed image,\n    and the loss curve (if provided). Saves the plot if a filename is provided.\n\n    Parameters\n    ----------\n    axs (numpy array): Array of matplotlib axes to plot on.\n    mri_2D (torch.Tensor): Original MRI image.\n    traj : Trajectory.\n    recon (torch.Tensor): Reconstructed image after training.\n    loss (list, optional): List of loss values to plot. Defaults to None.\n    save_name (str, optional): Filename to save the plot. Defaults to None.\n    \"\"\"\n    axs = axs.flatten()\n    axs[0].imshow(np.abs(mri_2D[0]), cmap=\"gray\")\n    axs[0].axis(\"off\")\n    axs[0].set_title(\"MR Image\")\n    axs[1].scatter(*traj.T, s=0.5)\n    axs[1].set_title(\"Trajectory\")\n    axs[2].imshow(np.abs(recon[0][0].detach().cpu().numpy()), cmap=\"gray\")\n    axs[2].axis(\"off\")\n    axs[2].set_title(\"Reconstruction\")\n    if loss is not None:\n        axs[3].plot(loss)\n        axs[3].grid(\"on\")\n        axs[3].set_title(\"Loss\")\n    if save_name is not None:\n        plt.savefig(save_name, bbox_inches=\"tight\")\n        plt.close()\n    else:\n        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Setup Inputs (models, trajectory and image)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "init_traj = initialize_2D_cones(32, 256).reshape(-1, 2).astype(np.float32)\nmodel = Model(init_traj)\nmodel.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Get the image on which we will train our U-Net Model\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mri_2D = torch.Tensor(np.flipud(bwdl.get_mri(4, \"T1\")[80, ...]).astype(np.complex64))[\n    None\n]\nmri_2D = mri_2D / torch.mean(mri_2D)\nkspace_mri_2D = model.operator.op(mri_2D)\n\n# Before training, here is the simple reconstruction we have using a\n# density compensated adjoint.\ndc_adjoint = model.operator.adj_op(kspace_mri_2D)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "num_epochs = 100\noptimizer = torch.optim.RAdam(model.parameters(), lr=1e-3)\nmodel.train()\n\nfig, axs = plt.subplots(2, 2, figsize=(10, 10))\nfig.suptitle(\"Training Starting\")\naxs = axs.flatten()\n\naxs[0].imshow(np.abs(mri_2D[0]), cmap=\"gray\")\naxs[0].axis(\"off\")\naxs[0].set_title(\"MR Image\")\n\naxs[1].scatter(*init_traj.T, s=0.5)\naxs[1].set_title(\"Trajectory\")\n\nrecon_im = axs[2].imshow(\n    np.abs(dc_adjoint[0][0].detach().cpu().numpy()),\n    cmap=\"gray\",\n)\naxs[2].axis(\"off\")\naxs[2].set_title(\"Reconstruction\")\n(loss_curve,) = axs[3].plot([], [])\naxs[3].grid()\naxs[3].set_xlabel(\"epochs\")\naxs[3].set_ylabel(\"loss\")\n\nfig.tight_layout()\n\n\ndef train():\n    \"\"\"Train loop.\"\"\"\n    losses = []\n    for i in range(num_epochs):\n        out = model(kspace_mri_2D)  # Forward pass\n\n        loss = torch.nn.functional.l1_loss(out, mri_2D[None])  # Compute loss\n\n        optimizer.zero_grad()  # Zero gradients\n        loss.backward()  # Backward pass\n        optimizer.step()  # Update weights\n        losses.append(loss.item())\n        yield out.detach().cpu().numpy().squeeze(), losses\n\n\ndef plot_epoch(data):\n    img, losses = data\n    cur_epoch = len(losses)\n    recon_im.set_data(abs(img))\n    loss_curve.set_xdata(np.arange(cur_epoch))\n    loss_curve.set_ydata(losses)\n    axs[3].set_xlim(0, cur_epoch)\n    axs[3].set_ylim(0, 1.1 * max(losses))\n    axs[2].set_title(f\"Reconstruction, frame {cur_epoch}/{num_epochs}\")\n\n    if cur_epoch < num_epochs:\n        fig.suptitle(\"Training in progress \" + \".\" * (1 + cur_epoch % 3))\n    else:\n        fig.suptitle(\"Training complete !\")\n\n\nani = animation.FuncAnimation(\n    fig, plot_epoch, train, save_count=num_epochs, repeat=False\n)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References\n\n.. [fastmri] O. Ronneberger, P. Fischer, and Thomas Brox. U-net: Convolutional networks\n          for biomedical image segmentation. In International Conference on Medical\n          image computing and computer-assisted intervention, pages 234\u2013241.\n          Springer, 2015.\n          https://github.com/facebookresearch/fastMRI/blob/main/fastmri/models/unet.py\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}