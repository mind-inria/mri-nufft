{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class='alert alert-warning'>\n\n# Need GPU warning\n\nRunning this mri-nufft example requires a GPU, and hence is NOT possible on binder currently We request you to kindly run this notebook on Google Colab by clicking the link below. Additionally, please make sure to set the runtime on Colab to use a GPU and install the below libraries before running.\n</div>\n<div class=\"colab-button\">\n            <a href=\"https://colab.research.google.com/github/mind-inria/mri-nufft/blob/gh-pages/examples/generated/autoexamples/GPU/example_learn_samples_multicoil.ipynb\" target=\"_blank\">\n                <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" \n                alt=\"Open In Colab\"/>\n            </a>\n        </div>\n        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Learn Sampling pattern for multi-coil MRI\n\nA small pytorch example to showcase learning k-space sampling patterns.\nThis example showcases the auto-diff capabilities of the NUFFT operator\nwrt to k-space trajectory in mri-nufft.\n\nBriefly, in this example we try to learn the k-space samples $\\mathbf{K}$ for the following cost function:\n\n\\begin{align}\\mathbf{\\hat{K}} =  arg \\min_{\\mathbf{K}} ||  \\sum_{\\ell=1}^LS_\\ell^* \\mathcal{F}_\\mathbf{K}^* D_\\mathbf{K} \\mathcal{F}_\\mathbf{K} x_\\ell - \\mathbf{x}_{sos} ||_2^2\\end{align}\n\nwhere $S_\\ell$ is the sensitivity map for the $\\ell$-th coil, $\\mathcal{F}_\\mathbf{K}$ is the forward NUFFT operator and $D_\\mathbf{K}$ is the density compensators for trajectory $\\mathbf{K}$,  $\\mathbf{x}_\\ell$ is the image for the $\\ell$-th coil, and $\\mathbf{x}_{sos} = \\sqrt{\\sum_{\\ell=1}^L x_\\ell^2}$ is the sum-of-squares image as target image to be reconstructed.\n\nIn this example, the forward NUFFT operator $\\mathcal{F}_\\mathbf{K}$ is implemented with `model.operator` while the SENSE operator ``model.sense_op`` models the term $\\mathbf{A} = \\sum_{\\ell=1}^LS_\\ell^* \\mathcal{F}_\\mathbf{K}^* D_\\mathbf{K}$.\nFor our data, we use a 2D slice of a 3D MRI image from the BrainWeb dataset, and the sensitivity maps are simulated using the `birdcage_maps` function from `sigpy.mri`.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>To showcase the features of ``mri-nufft``, we use ``\n    \"cufinufft\"`` backend for ``model.operator`` without density compensation and ``\"gpunufft\"`` backend for ``model.sense_op`` with density compensation.</p></div>\n\n<div class=\"alert alert-danger\"><h4>Warning</h4><p>This example only showcases the autodiff capabilities, the learned sampling pattern is not scanner compliant as the scanner gradients required to implement it violate the hardware constraints. In practice, a projection $\\Pi_\\mathcal{Q}(\\mathbf{K})$ into the scanner constraints set $\\mathcal{Q}$ is recommended (see [Proj]_). This is implemented in the proprietary SPARKLING package [Sparks]_. Users are encouraged to contact the authors if they want to use it.</p></div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Install libraries\n!pip install mri-nufft[gpunufft] cufinufft sigpy scikit-image\n!pip install brainweb-dl  # Required for data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\n\nimport brainweb_dl as bwdl\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport matplotlib.animation as animation\n\nfrom mrinufft import get_operator\nfrom mrinufft.extras import get_smaps\nfrom mrinufft.trajectories import initialize_2D_radial\nfrom sigpy.mri import birdcage_maps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup a simple class to learn trajectory\n<div class=\"alert alert-info\"><h4>Note</h4><p>While we are only learning the NUFFT operator, we still need the gradient `wrt_data=True` to have all the gradients computed correctly.\n    See [Projector]_ for more details.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "BACKEND = os.environ.get(\"MRINUFFT_BACKEND\", \"cufinufft\")\nplt.rcParams[\"animation.embed_limit\"] = 2**30  # 1GiB is very large.\n\n\nclass Model(torch.nn.Module):\n    def __init__(self, inital_trajectory, n_coils, img_size=(256, 256)):\n        super(Model, self).__init__()\n        self.trajectory = torch.nn.Parameter(\n            data=torch.Tensor(inital_trajectory),\n            requires_grad=True,\n        )\n        sample_points = inital_trajectory.reshape(-1, inital_trajectory.shape[-1])\n        # A simple acquisition model simulated with a forward NUFFT operator. We dont need density compensation here.\n        # The trajectory is scaled by 2*pi for cufinufft backend.\n        self.operator = get_operator(BACKEND, wrt_data=True, wrt_traj=True)(\n            sample_points * 2 * np.pi,\n            shape=img_size,\n            n_coils=n_coils,\n            squeeze_dims=False,\n        )\n        # A simple density compensated adjoint SENSE operator with sensitivity maps `smaps`.\n        self.sense_op = get_operator(BACKEND, wrt_data=True, wrt_traj=True)(\n            sample_points,\n            shape=img_size,\n            density=True,\n            n_coils=n_coils,\n            smaps=np.ones(\n                (n_coils, *img_size), dtype=np.complex64\n            ),  # Dummy smaps, this is updated in forward pass\n            squeeze_dims=False,\n        )\n        self.img_size = img_size\n\n    def forward(self, x):\n        \"\"\"Forward pass of the model.\"\"\"\n        # Update the trajectory in the NUFFT operator.\n        # The trajectory is scaled by 2*pi for cufinufft backend.\n        # Note that the re-computation of density compensation happens internally.\n        self.operator.samples = self.trajectory.clone() * 2 * np.pi\n        self.sense_op.samples = self.trajectory.clone()\n\n        # Simulate the acquisition process\n        kspace = self.operator.op(x)\n\n        # Recompute the sensitivity maps for the updated trajectory.\n        self.sense_op.smaps, _ = get_smaps(\"low_frequency\")(\n            self.trajectory.detach().numpy(),\n            self.img_size,\n            kspace.detach(),\n            backend=BACKEND,\n            density=self.sense_op.density,\n            blurr_factor=20,\n        )\n        # Reconstruction using the sense operator\n        adjoint = self.sense_op.adj_op(kspace).abs()\n        return adjoint / torch.mean(adjoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup model and optimizer\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "num_epochs = 100\n\nn_coils = 6\ninit_traj = (\n    initialize_2D_radial(32, 256).astype(np.float32).reshape(-1, 2).astype(np.float32)\n)\nmodel = Model(init_traj, n_coils=n_coils, img_size=(256, 256))\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\nschedulder = torch.optim.lr_scheduler.LinearLR(\n    optimizer,\n    start_factor=1,\n    end_factor=0.1,\n    total_iters=num_epochs,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup data\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mri_2D = torch.from_numpy(np.flipud(bwdl.get_mri(4, \"T1\")[80, ...]).astype(np.float32))\nmri_2D = mri_2D / torch.mean(mri_2D)\nsmaps_simulated = torch.from_numpy(birdcage_maps((n_coils, *mri_2D.shape)))\nmcmri_2D = mri_2D[None].to(torch.complex64) * smaps_simulated\n\n\nmodel.eval()\nrecon = model(mcmri_2D)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training and plotting\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(2, 2, figsize=(10, 10))\nfig.suptitle(\"Training Starting\")\naxs = axs.flatten()\n\naxs[0].imshow(np.abs(mri_2D), cmap=\"gray\")\naxs[0].axis(\"off\")\naxs[0].set_title(\"MR Image\")\n\ntraj_scat = axs[1].scatter(*init_traj.T, s=0.5)\naxs[1].set_title(\"Trajectory\")\n\nrecon_im = axs[2].imshow(np.abs(recon.squeeze().detach().cpu().numpy()), cmap=\"gray\")\naxs[2].axis(\"off\")\naxs[2].set_title(\"Reconstruction\")\n(loss_curve,) = axs[3].plot([], [])\naxs[3].grid()\naxs[3].set_xlabel(\"epochs\")\naxs[3].set_ylabel(\"loss\")\n\nfig.tight_layout()\n\n\ndef train():\n    \"\"\"Train loop.\"\"\"\n    losses = []\n    for i in range(num_epochs):\n        out = model(mcmri_2D)\n        loss = torch.nn.functional.mse_loss(out, mri_2D[None, None])  # Compute loss\n\n        optimizer.zero_grad()  # Zero gradients\n        loss.backward()  # Backward pass\n        optimizer.step()  # Update weights\n        with torch.no_grad():\n            # clamp the value of trajectory between [-0.5, 0.5]\n            for param in model.parameters():\n                param.clamp_(-0.5, 0.5)\n        schedulder.step()\n        losses.append(loss.item())\n        yield (\n            out.detach().cpu().numpy().squeeze(),\n            model.trajectory.detach().cpu().numpy(),\n            losses,\n        )\n\n\ndef plot_epoch(data):\n    img, traj, losses = data\n\n    cur_epoch = len(losses)\n    recon_im.set_data(abs(img))\n    loss_curve.set_xdata(np.arange(cur_epoch))\n    loss_curve.set_ydata(losses)\n    traj_scat.set_offsets(traj)\n\n    axs[3].set_xlim(0, cur_epoch)\n    axs[3].set_ylim(0, 1.1 * max(losses))\n    axs[2].set_title(f\"Reconstruction, frame {cur_epoch}/{num_epochs}\")\n    axs[1].set_title(f\"Trajectory, frame {cur_epoch}/{num_epochs}\")\n\n    if cur_epoch < num_epochs:\n        fig.suptitle(\"Training in progress \" + \".\" * (1 + cur_epoch % 3))\n    else:\n        fig.suptitle(\"Training complete !\")\n\n\nani = animation.FuncAnimation(\n    fig, plot_epoch, train, save_count=num_epochs, repeat=False\n)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### References\n\n.. [Proj] N. Chauffert, P. Weiss, J. Kahn and P. Ciuciu, \"A Projection Algorithm for\n          Gradient Waveforms Design in Magnetic Resonance Imaging,\" in\n          IEEE Transactions on Medical Imaging, vol. 35, no. 9, pp. 2026-2039, Sept. 2016,\n          doi: 10.1109/TMI.2016.2544251.\n.. [Sparks] G. R. Chaithya, P. Weiss, G. Daval-Frérot, A. Massire, A. Vignaud and P. Ciuciu,\n          \"Optimizing Full 3D SPARKLING Trajectories for High-Resolution Magnetic\n          Resonance Imaging,\" in IEEE Transactions on Medical Imaging, vol. 41, no. 8,\n          pp. 2105-2117, Aug. 2022, doi: 10.1109/TMI.2022.3157269.\n.. [Projector] Chaithya GR, and Philippe Ciuciu. 2023. \"Jointly Learning Non-Cartesian\n          k-Space Trajectories and Reconstruction Networks for 2D and 3D MR Imaging\n          through Projection\" Bioengineering 10, no. 2: 158.\n          https://doi.org/10.3390/bioengineering10020158\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}