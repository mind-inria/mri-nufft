{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Learning sampling pattern with decimation\n\nAn example using PyTorch to showcase learning k-space sampling patterns with decimation.\n\nThis example showcases the auto-differentiation capabilities of the NUFFT operator\nwith respect to the k-space trajectory in MRI-nufft.\n\nHereafter we learn the k-space sample locations $\\mathbf{K}$ using the following cost function:\n\n\\begin{align}\\mathbf{\\hat{K}} =  arg \\min_{\\mathbf{K}} ||  \\mathcal{F}_\\mathbf{K}^* D_\\mathbf{K} \\mathcal{F}_\\mathbf{K} \\mathbf{x} - \\mathbf{x} ||_2^2\\end{align}\n\nwhere $\\mathcal{F}_\\mathbf{K}$ is the forward NUFFT operator,\n$D_\\mathbf{K}$ is the density compensator for trajectory $\\mathbf{K}$,\nand $\\mathbf{x}$ is the MR image which is also the target image to be reconstructed.\n\nAdditionally, in order to converge faster, we also learn the trajectory in a multi-resolution fashion.\nThis is done by first optimizing x8 times decimated trajectory locations, called control points.\nAfter a fixed number of iterations (5 in this example), these control points are upscaled by a factor of 2.\nNote that the NUFFT operator always holds linearly interpolated version of the control points as k-space sampling trajectory.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>This example can run on a binder instance as it is purely CPU based backend (finufft), and is restricted to a 2D single coil toy case.</p></div>\n\n<div class=\"alert alert-danger\"><h4>Warning</h4><p>This example only showcases the auto-differentiation capabilities, the learned sampling pattern\n    is not scanner compliant as the gradients required to implement it violate the hardware constraints.\n    In practice, a projection $\\Pi_\\mathcal{Q}(\\mathbf{K})$ onto the scanner constraints set $\\mathcal{Q}$ is recommended\n    (see [Cha+16]_). This is implemented in the proprietary SPARKLING package [Cha+22]_.\n    Users are encouraged to contact the authors if they want to use it.</p></div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class='alert alert-info'>\n\n# Install libraries needed for Colab\n\nThe below installation commands are needed to be run only on Google Colab.\n</div>\n<div class=\"colab-button\">\n            <a href=\"https://colab.research.google.com/github/mind-inria/mri-nufft/blob/gh-pages/examples/generated/autoexamples/trajectories/example_learn_samples_multires.ipynb\" target=\"_blank\">\n                <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" \n                alt=\"Open In Colab\"/>\n            </a>\n        </div>\n        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Install libraries\n!pip install mri-nufft[finufft]\n!pip install brainweb-dl  # Required for data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\nimport brainweb_dl as bwdl\nfrom matplotlib import animation\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\n\nfrom mrinufft import get_operator\nfrom mrinufft.trajectories import initialize_2D_radial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Utils\n\n### Model class\n<div class=\"alert alert-info\"><h4>Note</h4><p>While we are only learning the NUFFT operator, we still need the gradient `wrt_data=True` to have all the gradients computed correctly.\n    See [GRC23]_ for more details.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "BACKEND = os.environ.get(\"MRINUFFT_BACKEND\", \"finufft\")\n\n\nplt.rcParams[\"animation.embed_limit\"] = 2**30  # 1GiB is very large.\n\n\nclass Model(torch.nn.Module):\n    def __init__(\n        self,\n        inital_trajectory,\n        img_size=(256, 256),\n        start_decim=8,\n        interpolation_mode=\"linear\",\n    ):\n        super().__init__()\n        self.control = torch.nn.Parameter(\n            data=torch.Tensor(inital_trajectory[:, ::start_decim]),\n            requires_grad=True,\n        )\n        self.current_decim = start_decim\n        self.interpolation_mode = interpolation_mode\n        sample_points = inital_trajectory.reshape(-1, inital_trajectory.shape[-1])\n        self.operator = get_operator(BACKEND, wrt_data=True, wrt_traj=True)(\n            sample_points,\n            shape=img_size,\n            density=True,\n            squeeze_dims=False,\n        )\n        self.img_size = img_size\n\n    def _interpolate(self, traj, factor=2):\n        \"\"\"Torch interpolate function to upsample the trajectory\"\"\"\n        return torch.nn.functional.interpolate(\n            traj.moveaxis(1, -1),\n            scale_factor=factor,\n            mode=self.interpolation_mode,\n            align_corners=True,\n        ).moveaxis(-1, 1)\n\n    def get_trajectory(self):\n        \"\"\"Function to get trajectory, which is interpolated version of control points.\"\"\"\n        traj = self.control.clone()\n        for i in range(np.log2(self.current_decim).astype(int)):\n            traj = self._interpolate(traj)\n\n        return traj.reshape(-1, traj.shape[-1])\n\n    def upscale(self, factor=2):\n        \"\"\"Upscaling the model.\n        In this step, the number of control points are doubled and interpolated.\n        \"\"\"\n        self.control = torch.nn.Parameter(\n            data=self._interpolate(self.control),\n            requires_grad=True,\n        )\n        self.current_decim /= factor\n\n    def forward(self, x):\n        traj = self.get_trajectory()\n        self.operator.samples = traj\n\n        # Simulate the acquisition process\n        kspace = self.operator.op(x)\n\n        adjoint = self.operator.adj_op(kspace).abs()\n        return adjoint / torch.mean(adjoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Optimizer upscaling\n\nThe multi-resolution training requires us to update the optimizer as well. The optimization weights will also be\nlinearly interpolated.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def upsample_optimizer(optimizer, new_optimizer, factor=2):\n    \"\"\"Upsample the optimizer.\"\"\"\n    for old_group, new_group in zip(optimizer.param_groups, new_optimizer.param_groups):\n        for old_param, new_param in zip(old_group[\"params\"], new_group[\"params\"]):\n            # Interpolate optimizer states\n            if old_param in optimizer.state:\n                for key in optimizer.state[old_param].keys():\n                    if isinstance(optimizer.state[old_param][key], torch.Tensor):\n                        old_state = optimizer.state[old_param][key]\n                        if old_state.ndim == 0:\n                            new_state = old_state\n                        else:\n                            new_state = torch.nn.functional.interpolate(\n                                old_state.moveaxis(1, -1),\n                                scale_factor=factor,\n                                mode=\"linear\",\n                            ).moveaxis(-1, 1)\n                        new_optimizer.state[new_param][key] = new_state\n                    else:\n                        new_optimizer.state[new_param][key] = optimizer.state[\n                            old_param\n                        ][key]\n    return new_optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data preparation\n\nA single image to train the model over. Note that in practice\nwe would use a whole dataset instead (e.g. fastMRI).\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "volume = np.flip(bwdl.get_mri(4, \"T1\"), axis=(0, 1, 2))\nimage = torch.from_numpy(volume[-80, ...].astype(np.float32))[None]\nimage = image / torch.mean(image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A basic radial trajectory with an acceleration factor of 8.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "AF = 8\ninitial_traj = initialize_2D_radial(image.shape[1] // AF, image.shape[2]).astype(\n    np.float32\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Trajectory learning\n\n### Initialisation\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "N_upscale = 4\n\nmodel = Model(initial_traj, img_size=image.shape[1:], start_decim=2 ** (N_upscale - 1))\nmodel = model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The image obtained before learning the sampling pattern\nis highly degraded because of the acceleration factor and simplicity\nof the trajectory.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "initial_recons = model(image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training loop\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\nmodel.train()\nnum_epochs = 30\n\n\n# setup plotting\nfig, axs = plt.subplots(2, 2, figsize=(10, 10))\nfig.suptitle(\"Training Starting\")\naxs = axs.flatten()\n\naxs[0].imshow(np.abs(image.detach().cpu().numpy().squeeze()), cmap=\"gray\")\naxs[0].axis(\"off\")\naxs[0].set_title(\"MR Image\")\n\ntraj_scat = axs[1].scatter(\n    *model.get_trajectory().detach().cpu().numpy().T, s=0.5, c=\"tab:blue\"\n)\ntraj_scat2 = axs[1].scatter(*model.control.detach().cpu().numpy().T, s=2, c=\"tab:red\")\n\naxs[1].legend([\"Trajectory\", \"Control Points\"], loc=\"upper right\")\naxs[1].set_title(\"Trajectory\")\n\nrecon_im = axs[2].imshow(\n    np.abs(initial_recons.squeeze().detach().cpu().numpy()), cmap=\"gray\"\n)\naxs[2].axis(\"off\")\naxs[2].set_title(\"Reconstruction\")\n(loss_curve,) = axs[3].plot([], [])\naxs[3].grid()\naxs[3].set_xlim(0, 1)\naxs[3].set_xlabel(\"epochs\")\naxs[3].set_ylabel(\"loss\")\n# add line marking the decimation steps\n[\n    axs[3].axvline(num_epochs * i, c=\"tab:red\", linestyle=\"dashed\")\n    for i in range(N_upscale)\n]\nfig.tight_layout()\n\n\ndef train():\n    global optimizer\n    losses = []\n    while model.current_decim >= 1:\n        for _ in range(num_epochs):\n            out = model(image)\n            loss = torch.nn.functional.mse_loss(out, image[None, None])\n            losses.append(loss.item())\n            optimizer.zero_grad()\n            loss.backward()\n\n            optimizer.step()\n            with torch.no_grad():\n                # Clamp the value of trajectory between [-0.5, 0.5]\n                for param in model.parameters():\n                    param.clamp_(-0.5, 0.5)\n\n            yield (\n                out.detach().cpu().numpy(),\n                model.get_trajectory().detach().cpu().numpy(),\n                model.control.detach().cpu().numpy(),\n                losses,\n                model.current_decim,\n            )\n\n        if model.current_decim == 1:\n            break\n        else:\n            model.upscale()\n            optimizer = upsample_optimizer(\n                optimizer, torch.optim.Adam(model.parameters(), lr=1e-3)\n            )\n\n\ndef plot_epoch(data):\n    recon, traj, control, losses, decim = data\n    cur_epoch = len(losses)\n    recon_im.set_data(abs(recon).squeeze())\n    loss_curve.set_xdata(np.arange(cur_epoch))\n    loss_curve.set_ydata(losses)\n    traj_scat.set_offsets(traj)\n\n    axs[3].set_xlim(0, cur_epoch)\n    axs[3].set_ylim(0, 1.1 * max(losses))\n    axs[2].set_title(f\"Reconstruction, frame {cur_epoch}/{num_epochs*N_upscale}\")\n    axs[1].set_title(\n        f\"Trajectory, step {cur_epoch}/{num_epochs * N_upscale}, decim = {decim}\"\n    )\n\n    traj_scat.set_offsets(traj.reshape(-1, 2))\n    traj_scat2.set_offsets(control.reshape(-1, 2))\n\n    if cur_epoch < num_epochs * N_upscale:\n        fig.suptitle(\"Training in progress \" + \".\" * (1 + cur_epoch % 3))\n    else:\n        fig.suptitle(\"Training complete !\")\n\n\nani = animation.FuncAnimation(\n    fig, plot_epoch, train, repeat=False, save_count=num_epochs, interval=50\n)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The learned trajectory above improves the reconstruction quality as compared to\nthe initial trajectory shown above. Note of course that the reconstructed\nimage is far from perfect because of the documentation rendering constraints.\nIn order to improve the results one can start by training it for more than\njust 5 iterations per decimation level. Also density compensation should be used,\neven though it was avoided here for CPU compliance. Check out\n`sphx_glr_generated_autoexamples_GPU_example_learn_samples.py` to know more.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References\n\n.. [Cha+16] N. Chauffert, P. Weiss, J. Kahn and P. Ciuciu, \"A Projection Algorithm for\n          Gradient Waveforms Design in Magnetic Resonance Imaging,\" in\n          IEEE Transactions on Medical Imaging, vol. 35, no. 9, pp. 2026-2039, Sept. 2016,\n          doi: 10.1109/TMI.2016.2544251.\n.. [Cha+22] G. R. Chaithya, P. Weiss, G. Daval-Frérot, A. Massire, A. Vignaud and P. Ciuciu,\n          \"Optimizing Full 3D SPARKLING Trajectories for High-Resolution Magnetic\n          Resonance Imaging,\" in IEEE Transactions on Medical Imaging, vol. 41, no. 8,\n          pp. 2105-2117, Aug. 2022, doi: 10.1109/TMI.2022.3157269.\n.. [GRC23] Chaithya GR, and Philippe Ciuciu. 2023. \"Jointly Learning Non-Cartesian\n          k-Space Trajectories and Reconstruction Networks for 2D and 3D MR Imaging\n          through Projection\" Bioengineering 10, no. 2: 158.\n          https://doi.org/10.3390/bioengineering10020158\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}